{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14759599,"sourceType":"datasetVersion","datasetId":9433872}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#install requirements\n#The notebook relies on HuggingFace transformers for pretrained language models \n#and accelerate for compatibility and performance on Kaggle GPUs\n!pip install -q transformers accelerate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:52.707643Z","iopub.execute_input":"2026-02-07T16:36:52.708372Z","iopub.status.idle":"2026-02-07T16:36:56.096839Z","shell.execute_reply.started":"2026-02-07T16:36:52.708341Z","shell.execute_reply":"2026-02-07T16:36:56.096050Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"#import necessary modules\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, r2_score\n\nfrom transformers import DebertaV2Tokenizer, DebertaV2Model\n\n#We use DeBERTa-v3-base, a transformer-based language model pretrained \n#on large-scale corpora.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.098738Z","iopub.execute_input":"2026-02-07T16:36:56.099010Z","iopub.status.idle":"2026-02-07T16:36:56.103929Z","shell.execute_reply.started":"2026-02-07T16:36:56.098982Z","shell.execute_reply":"2026-02-07T16:36:56.103351Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"#Device Setup and Random Seeds\n#Automatically selects GPU (cuda) if available, otherwise CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.104763Z","iopub.execute_input":"2026-02-07T16:36:56.105032Z","iopub.status.idle":"2026-02-07T16:36:56.121021Z","shell.execute_reply.started":"2026-02-07T16:36:56.105010Z","shell.execute_reply":"2026-02-07T16:36:56.120481Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"#Load the training data\ndf = pd.read_csv(\"/kaggle/input/customer-complaints2/train_complaints.csv\")\ndf.columns = df.columns.str.strip().str.lower()\ndf.head() #check the data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.121823Z","iopub.execute_input":"2026-02-07T16:36:56.122032Z","iopub.status.idle":"2026-02-07T16:36:56.197406Z","shell.execute_reply.started":"2026-02-07T16:36:56.122012Z","shell.execute_reply":"2026-02-07T16:36:56.196782Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"   complaint_id                                     complaint_text  \\\n0       1634299  Back into XXXX of 2010 during this mortgage cr...   \n1       5505088  I checked my credit report and I am upset on w...   \n2      10979675  I am writing to dispute the accuracy of the in...   \n3       7520351  A transaction from XXXX XXXX XXXX submitted a ...   \n4       5847870  I was recently alerted to an account in collec...   \n\n                                    primary_category  \\\n0                                           Mortgage   \n1  Credit reporting, credit repair services, or o...   \n2  Credit reporting or other personal consumer re...   \n3                        Checking or savings account   \n4                                    Debt collection   \n\n                                  secondary_category  severity  \n0           Loan modification,collection,foreclosure         2  \n1  Problem with a credit reporting company's inve...         1  \n2  Problem with a company's investigation into an...         1  \n3                                Managing an account         1  \n4                  Attempts to collect debt not owed         5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>complaint_id</th>\n      <th>complaint_text</th>\n      <th>primary_category</th>\n      <th>secondary_category</th>\n      <th>severity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1634299</td>\n      <td>Back into XXXX of 2010 during this mortgage cr...</td>\n      <td>Mortgage</td>\n      <td>Loan modification,collection,foreclosure</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5505088</td>\n      <td>I checked my credit report and I am upset on w...</td>\n      <td>Credit reporting, credit repair services, or o...</td>\n      <td>Problem with a credit reporting company's inve...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10979675</td>\n      <td>I am writing to dispute the accuracy of the in...</td>\n      <td>Credit reporting or other personal consumer re...</td>\n      <td>Problem with a company's investigation into an...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7520351</td>\n      <td>A transaction from XXXX XXXX XXXX submitted a ...</td>\n      <td>Checking or savings account</td>\n      <td>Managing an account</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5847870</td>\n      <td>I was recently alerted to an account in collec...</td>\n      <td>Debt collection</td>\n      <td>Attempts to collect debt not owed</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"#Encode target labels\n#Convert categorical labels into numeric form.\n#This step prepares labels for loss functions used during training.\n\nprimary_encoder = LabelEncoder()\nsecondary_encoder = LabelEncoder()\n\ndf[\"primary_label\"] = primary_encoder.fit_transform(df[\"primary_category\"])\ndf[\"secondary_label\"] = secondary_encoder.fit_transform(df[\"secondary_category\"])\n\nnum_primary = df[\"primary_label\"].nunique()\nnum_secondary = df[\"secondary_label\"].nunique()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.199512Z","iopub.execute_input":"2026-02-07T16:36:56.199776Z","iopub.status.idle":"2026-02-07T16:36:56.207186Z","shell.execute_reply.started":"2026-02-07T16:36:56.199752Z","shell.execute_reply":"2026-02-07T16:36:56.206358Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"#Prepare text tokenizer compatible with the model.\ntokenizer = DebertaV2Tokenizer.from_pretrained(\n    \"microsoft/deberta-v3-base\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.208307Z","iopub.execute_input":"2026-02-07T16:36:56.208910Z","iopub.status.idle":"2026-02-07T16:36:56.833916Z","shell.execute_reply.started":"2026-02-07T16:36:56.208871Z","shell.execute_reply":"2026-02-07T16:36:56.833331Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"#Dataset class definition\n#returns pre-tokenized inputs\n#provides corresponding labels for all three tasks:\n#primary category\n#secondary category\n#severity\n\nclass ComplaintDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=224):\n        self.texts = df[\"complaint_text\"].tolist()\n        self.primary = df[\"primary_label\"].values\n        self.secondary = df[\"secondary_label\"].values\n        self.severity = df[\"severity\"].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"primary\": torch.tensor(self.primary[idx], dtype=torch.long),\n            \"secondary\": torch.tensor(self.secondary[idx], dtype=torch.long),\n            \"severity\": torch.tensor(self.severity[idx], dtype=torch.float),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.834795Z","iopub.execute_input":"2026-02-07T16:36:56.835066Z","iopub.status.idle":"2026-02-07T16:36:56.841521Z","shell.execute_reply.started":"2026-02-07T16:36:56.835037Z","shell.execute_reply":"2026-02-07T16:36:56.840858Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"#Multi-Task DeBERTa Model Definition\n#Define the neural network architecture.\n#This multi-task setup allows shared representations across related tasks.\nclass MultiTaskDeBERTa(nn.Module):\n    def __init__(self, num_primary, num_secondary):\n        super().__init__()\n\n        self.bert = DebertaV2Model.from_pretrained(\n            \"microsoft/deberta-v3-base\"\n           \n        )\n\n        hidden = self.bert.config.hidden_size\n        self.dropout = nn.Dropout(0.3)\n        #task specify heads-\n        self.primary_head = nn.Linear(hidden, num_primary)\n        self.secondary_head = nn.Linear(hidden, num_secondary)\n        self.severity_head = nn.Linear(hidden, 1)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        x = out.last_hidden_state[:, 0, :]\n        x = self.dropout(x)\n\n        return {\n            \"primary\": self.primary_head(x),\n            \"secondary\": self.secondary_head(x),\n            \"severity\": self.severity_head(x).squeeze(1)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.842565Z","iopub.execute_input":"2026-02-07T16:36:56.842836Z","iopub.status.idle":"2026-02-07T16:36:56.858029Z","shell.execute_reply.started":"2026-02-07T16:36:56.842809Z","shell.execute_reply":"2026-02-07T16:36:56.857472Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"#Dataloader initialization\n#Create a batch-wise data loader for training.\nn_folds = 4\nbatch_size = 8        # IMPORTANT: DeBERTa needs smaller batch\nepochs = 5\n\nskf = StratifiedKFold(\n    n_splits=n_folds,\n    shuffle=True,\n    random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.858851Z","iopub.execute_input":"2026-02-07T16:36:56.859173Z","iopub.status.idle":"2026-02-07T16:36:56.877516Z","shell.execute_reply.started":"2026-02-07T16:36:56.859121Z","shell.execute_reply":"2026-02-07T16:36:56.876797Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"#Loss function and optimizer\n#Purpose is to define how the model runs\n\n\nfor fold, (train_idx, val_idx) in enumerate(\n    skf.split(df, df[\"primary_label\"])\n):\n    print(f\"\\n===== Fold {fold+1}/{n_folds} =====\")\n\n    train_df = df.iloc[train_idx]\n    val_df   = df.iloc[val_idx]\n\n    train_ds = ComplaintDataset(train_df, tokenizer)\n    val_ds   = ComplaintDataset(val_df, tokenizer)\n\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True, num_workers=0\n    )\n    val_loader = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False, num_workers=0\n    )\n\n    model = MultiTaskDeBERTa(num_primary, num_secondary).to(device)\n\n    #Cross-entropy loss is used for both classification tasks\n    loss_p = nn.CrossEntropyLoss()\n    loss_s = nn.CrossEntropyLoss()\n    #Mean squared error (MSE) is used for severity regression\n    loss_v = nn.MSELoss()\n\n    #Loss weights align with the competition scoring metric.\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n    # ---- TRAIN ----\n    for epoch in range(epochs):\n        model.train()\n        for batch in train_loader:\n            optimizer.zero_grad()\n\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            p = batch[\"primary\"].to(device)\n            s = batch[\"secondary\"].to(device)\n            v = batch[\"severity\"].to(device)\n\n            out = model(input_ids, attention_mask)\n\n            lp = loss_p(out[\"primary\"], p)\n            ls = loss_s(out[\"secondary\"], s)\n            lv = loss_v(out[\"severity\"], v)\n\n            loss = 0.3*lp + 0.5*ls + 0.2*lv\n            loss.backward()\n            optimizer.step()\n\n   \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:36:56.878527Z","iopub.execute_input":"2026-02-07T16:36:56.878851Z","iopub.status.idle":"2026-02-07T17:20:46.749625Z","shell.execute_reply.started":"2026-02-07T16:36:56.878817Z","shell.execute_reply":"2026-02-07T17:20:46.748909Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n===== Fold 1/4 =====\n\n===== Fold 2/4 =====\n\n===== Fold 4/4 =====\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"#Final model training.\n#Train the final submission model on the full dataset.\n'''Training loop performs:\n\nforward pass\n\nloss computation\n\nbackpropagation\n\nweight updates'''\n\n\nfinal_model = MultiTaskDeBERTa(num_primary, num_secondary).to(device)\n\noptimizer = torch.optim.AdamW(final_model.parameters(), lr=2e-5)\n\nfull_ds = ComplaintDataset(df, tokenizer)\nfull_loader = DataLoader(full_ds, batch_size=batch_size, shuffle=True,num_workers=0)\n\nfinal_model.train()\nfor epoch in range(epochs):\n    for batch in full_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        p = batch[\"primary\"].to(device)\n        s = batch[\"secondary\"].to(device)\n        v = batch[\"severity\"].to(device)\n\n        out = final_model(input_ids, attention_mask)\n\n        loss = (\n            0.3*loss_p(out[\"primary\"], p)\n          + 0.4*loss_s(out[\"secondary\"], s)\n          + 0.2*loss_v(out[\"severity\"], v)\n        )\n\n        loss.backward()\n        optimizer.step()\ntorch.save(final_model.state_dict(), \"final_model.pt\")#save trained model. Ensures recovery in case of crash\n#This is the actual learning step that produces the model used for submission.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:20:46.750609Z","iopub.execute_input":"2026-02-07T17:20:46.750855Z","iopub.status.idle":"2026-02-07T17:35:22.898244Z","shell.execute_reply.started":"2026-02-07T17:20:46.750832Z","shell.execute_reply":"2026-02-07T17:35:22.897364Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#LOad the test data\ntest_df = pd.read_csv(\"/kaggle/input/customer-complaints2/test_complaints.csv\")\ntest_df.columns = test_df.columns.str.strip().str.lower()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:35:22.899635Z","iopub.execute_input":"2026-02-07T17:35:22.900344Z","iopub.status.idle":"2026-02-07T17:35:22.926423Z","shell.execute_reply.started":"2026-02-07T17:35:22.900302Z","shell.execute_reply":"2026-02-07T17:35:22.925865Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"#Tokenize the test data\nclass TestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=256):\n        self.texts = df[\"complaint_text\"].tolist()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:35:22.927343Z","iopub.execute_input":"2026-02-07T17:35:22.927604Z","iopub.status.idle":"2026-02-07T17:35:22.933559Z","shell.execute_reply.started":"2026-02-07T17:35:22.927576Z","shell.execute_reply":"2026-02-07T17:35:22.932861Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"#Create a dataset and loader for inference.\ntest_loader = DataLoader(\n    TestDataset(test_df, tokenizer),\n    batch_size=batch_size,\n    shuffle=False,#Batches are processed sequentially without shuffling.\n    num_workers=0\n)\n\nfinal_model.eval()\n#Generate predictions for all tasks\np_out, s_out, v_out = [], [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        out = final_model(\n            batch[\"input_ids\"].to(device),\n            batch[\"attention_mask\"].to(device)\n        )\n        p_out.extend(out[\"primary\"].argmax(1).cpu().numpy())\n        s_out.extend(out[\"secondary\"].argmax(1).cpu().numpy())\n        v_out.extend(out[\"severity\"].cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:35:22.936349Z","iopub.execute_input":"2026-02-07T17:35:22.936633Z","iopub.status.idle":"2026-02-07T17:35:34.097053Z","shell.execute_reply.started":"2026-02-07T17:35:22.936612Z","shell.execute_reply":"2026-02-07T17:35:34.096124Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"#Create the submission file\nsubmission = pd.DataFrame({\n    \"complaint_id\": test_df[\"complaint_id\"],\n    \"primary_category\": primary_encoder.inverse_transform(p_out),\n    \"secondary_category\": secondary_encoder.inverse_transform(s_out),\n    \"severity\": np.clip(np.rint(v_out), 1, 5).astype(int)#clipped to the valid range [1, 5]\n})\n\nsubmission.to_csv(\"submission4.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T17:42:50.499622Z","iopub.execute_input":"2026-02-07T17:42:50.499908Z","iopub.status.idle":"2026-02-07T17:42:50.509266Z","shell.execute_reply.started":"2026-02-07T17:42:50.499879Z","shell.execute_reply":"2026-02-07T17:42:50.508557Z"}},"outputs":[],"execution_count":49}]}